{
  "hash": "94efd120befea1fcd49a990de6488a94",
  "result": {
    "markdown": "---\ntitle: \"3\\\\. Machine Learning in Clustering\"\nauthor: \"Saim Ehtesham Ali\"\ndate: \"2023-11-22\"\ncategories: [Linear Regression, ML, ML Basics]\noutput: html_document\nimage: \"image.jpg\"\nformat:\n  html:\n    code-fold: true\ncode-fold: true\nkeep-ipynb: true\n---\n\n# **Machine Learning in Clustering: Understanding DBSCAN**\n\nClustering is a significant machine learning task that involves grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. One robust and widely-used method in clustering is DBSCAN (Density-Based Spatial Clustering of Applications with Noise). This blog post explores DBSCAN and demonstrates its application using a real-world dataset.\n\n## **DBSCAN in Machine Learning**\n\n-   **Handling Noise**: One of the key strengths of DBSCAN is its ability to identify and deal with noise in the data. It can effectively separate outliers from core groups in the dataset.\n\n-   **No Need to Specify Number of Clusters**: Unlike many clustering algorithms, DBSCAN doesn't require you to specify the number of clusters beforehand. It determines the number of clusters based on the data.\n\n-   **Flexibility in Cluster Shapes**: DBSCAN can find arbitrarily shaped clusters. It's not limited to finding spherical clusters like k-means, making it more versatile for real-world data.\n\n## **Example: DBSCAN Clustering on Public Dataset**\n\nFor this example, we will use a publicly available dataset and apply the DBSCAN algorithm to identify clusters within it. We'll visualize the results using a scatter plot, labeling the different clusters identified by DBSCAN.\n\n### **Python Code for DBSCAN Clustering**\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import datasets\n\n# Load a sample dataset (Iris dataset)\niris = datasets.load_iris()\nX = iris.data[:, :2]  # We only take the first two features for simplicity\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nclusters = dbscan.fit_predict(X)\n\n# Plotting\nplt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', label='Cluster Label')\nplt.title('DBSCAN Clustering on Iris Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=589 height=449}\n:::\n:::\n\n\nIn this example, we use the Iris dataset, a classic dataset in machine learning. We apply the **`DBSCAN`** algorithm from Scikit-learn, specifying an epsilon value for the neighborhood size and the minimum number of samples required to form a cluster. The result is visualized in a scatter plot, where each color represents a different cluster as identified by DBSCAN.\n\n## **Understanding the Scatter Plot**\n\nThe scatter plot demonstrates how DBSCAN clusters the data points. Each color in the plot represents a different cluster, while outliers (points not belonging to any cluster) can also be identified. This visualization provides a clear understanding of how DBSCAN segregates data into distinct groups and identifies noise, offering insights into the structure of the data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}