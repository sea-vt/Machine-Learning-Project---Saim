{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"4\\\\. Machine Learning in Classification\"\n",
        "author: \"Saim Ehtesham Ali\"\n",
        "date: \"2023-11-22\"\n",
        "categories: [Classification, ML, ML Basics, ROC, PR, Confusion Matrix]\n",
        "output: html_document\n",
        "image: \"lm 4.png\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "code-fold: true\n",
        "keep-ipynb: true\n",
        "---"
      ],
      "id": "c4894ec5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Machine Learning in Classification: ROC, PR, and Confusion Matrix**\n",
        "\n",
        "Classification is a core task in machine learning where the objective is to predict discrete categories or class labels. To evaluate the performance of classification models, several tools are used, including ROC curves, PR curves, and confusion matrices. This blog post delves into these evaluation metrics and demonstrates their use with examples.\n",
        "\n",
        "## **ROC (Receiver Operating Characteristic)**\n",
        "\n",
        "### **Understanding ROC**\n",
        "\n",
        "The ROC curve is a graphical representation of a classification model's performance. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. The Area Under the Curve (AUC) of the ROC curve is a single value summary of the model performance.\n",
        "\n",
        "### **Example: ROC Curve Using Iris Dataset**\n"
      ],
      "id": "38d040ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Binary classification problem: Classify Iris-setosa vs others\n",
        "y = (y == 0).astype(int) \n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting ROC Curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "id": "6ec33afe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the Iris dataset to perform binary classification (Iris-setosa vs others). We train a logistic regression model and plot the ROC curve, along with calculating the AUC to evaluate the model's performance.\n",
        "\n",
        "## **PR (Precision-Recall)**\n",
        "\n",
        "### **Understanding PR**\n",
        "\n",
        "The Precision-Recall (PR) curve is another tool for evaluating classifiers, especially useful in imbalanced datasets. It plots precision (positive predictive value) against recall (true positive rate).\n",
        "\n",
        "### **Example: PR Curve Using Breast Cancer Dataset**\n"
      ],
      "id": "c416658e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Logistic Regression Model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "id": "e7774078",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example uses the Breast Cancer dataset to demonstrate the PR curve. A logistic regression model is trained, and the PR curve is plotted to assess the trade-off between precision and recall for different threshold values.\n",
        "\n",
        "## **Confusion Matrix**\n",
        "\n",
        "### **Understanding Confusion Matrix**\n",
        "\n",
        "A confusion matrix is a table used to evaluate the performance of a classification model. It shows the actual versus predicted classifications and breaks down the results into true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "### **Example: Confusion Matrix Using Wine Dataset**\n"
      ],
      "id": "b5eb25d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generating the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualization\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "id": "28b237e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we use the Wine dataset to demonstrate the confusion matrix. After training a Random Forest Classifier, the confusion matrix is plotted using Seaborn, providing a clear visualization of the model's performance in terms of correct and incorrect classifications.\n",
        "\n",
        "## **Concluding Thoughts**\n",
        "\n",
        "The tools and examples discussed in this blog post provide a comprehensive overview of evaluating classification models in machine learning. The ROC and PR curves offer insights into the model's performance from different perspectives, particularly useful in varied dataset conditions. The confusion matrix, on the other hand, provides a straightforward, quantifiable measure of a model's predictive capabilities and errors.\n",
        "\n",
        "When applied correctly, these tools can greatly assist in understanding the strengths and weaknesses of classification models, guiding data scientists in model selection, tuning, and improvement. As machine learning continues to evolve, the proper evaluation of models remains a cornerstone of developing robust, reliable, and efficient predictive systems.\n",
        "\n",
        "Remember, the journey in machine learning is as much about understanding and interpreting the models as it is about building them."
      ],
      "id": "fb48690e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}