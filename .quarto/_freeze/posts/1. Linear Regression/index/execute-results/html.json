{
  "hash": "bed393adc8879ae5dcaf733dbb033b86",
  "result": {
    "markdown": "---\ntitle: \"1\\\\. Machine Learning in Linear Regression\"\nauthor: \"Saim Ehtesham Ali\"\ndate: \"2023-11-22\"\ncategories: [Linear Regression, ML, ML Basics]\noutput: html_document\nimage: \"ml 1.png\"\nformat:\n  html:\n    code-fold: true\ncode-fold: true\nkeep-ipynb: true\n---\n\n# **Understanding Linear Regression**\n\nLinear regression is a widely used statistical method that models the relationship between a dependent variable and one or more independent variables. It's based on the assumption that there is a linear relationship between these variables. The goal of linear regression is to find the best-fitting straight line through the data points. This line is represented by an equation that predicts the dependent variable based on the values of the independent variables. This technique is particularly useful because of its simplicity and efficiency in forecasting outcomes.\n\n## **Mathematical Background of Linear Regression**\n\nLinear regression is a fundamental statistical approach in predictive modeling and machine learning. It operates on the principle that there's a linear relationship between the dependent variable (often denoted as �y) and one or more independent variables (denoted as �X).\n\n### **Equation of a Linear Model**\n\nThe general form of a simple linear regression model (with one independent variable) is:\n\n�=�0+�1�+�y=β0​+β1​X+ϵ\n\nHere, �y is the dependent variable, �X is the independent variable, �0β0​ is the y-intercept, �1β1​ is the slope of the line, and �ϵ represents the error term.\n\n### **Estimating the Coefficients**\n\nThe coefficients �0β0​ and �1β1​ are estimated using the method of least squares. This method minimizes the sum of the squares of the differences between the observed values and the values predicted by the model. The formulae for these coefficients in a simple linear regression are:\n\n�1=∑�=1�(��−�ˉ)(��−�ˉ)∑�=1�(��−�ˉ)2β1​=∑i=1n​(Xi​−Xˉ)2∑i=1n​(Xi​−Xˉ)(Yi​−Yˉ)​ �0=�ˉ−�1�ˉβ0​=Yˉ−β1​Xˉ\n\nWhere �ˉXˉ and �ˉYˉ are the mean values of �X and �Y respectively.\n\n### **Assessing the Model**\n\nThe goodness of fit for the linear regression model is typically assessed using the coefficient of determination, denoted as �2R2. It measures the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n\n�2=1−����������R2=1−SStot​SSres​​\n\nWhere �����SSres​ is the sum of squares of residuals, and �����SStot​ is the total sum of squares.\n\n## **Advantages of Linear Regression**\n\n-   **Simplicity**: Linear regression is straightforward to implement, making it a great starting point for predictive modeling. Even with minimal statistical knowledge, one can interpret and understand the results of a linear regression model. This simplicity also leads to ease in training and predicting with these models, which is beneficial in many practical applications.\n\n-   **Interpretability**: The results of linear regression models are highly interpretable, as they provide clear and actionable insights. Each coefficient in the linear equation represents the impact of one independent variable on the dependent variable, allowing for straightforward interpretation of how each factor influences the outcome.\n\n-   **Basis for Other Methods**: Linear regression forms the foundation for many other statistical and machine learning techniques. Understanding linear regression is crucial before moving on to more complex models. It provides the fundamental concepts used in other advanced techniques, such as logistic regression and neural networks.\n\n## **Uses of Linear Regression**\n\n-   **Predictive Analysis**: Linear regression is extensively used in various fields for predictive analysis. In finance, it helps in predicting stock prices, in healthcare, it's used to anticipate disease progression, and in real estate, it can predict property prices. Its ability to model and forecast continuous variables makes it invaluable in these fields.\n\n-   **Risk Assessment**: The model is pivotal in insurance and finance for risk assessment. It helps in understanding and quantifying the risks associated with various factors. For instance, it can be used to determine the risk factors in car insurance premiums or to evaluate credit risk in banking.\n\n-   **Trend Analysis**: In economics and business, linear regression is used for trend analysis. It helps in identifying and quantifying trends in sales, market growth, or economic changes. This information is crucial for strategic planning and decision-making in business environments.\n\n## **Example 1: Simple Linear Regression**\n\nThe following example in Python showcases how to implement a linear regression model. We'll generate some synthetic data, fit a linear regression model, and then visualize the results with a plot.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate some random data\nnp.random.seed(0)\nx = np.random.rand(100, 1)\ny = 2 + 3 * x + np.random.randn(100, 1)\n\n# Fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(x, y)\ny_pred = model.predict(x)\n\n# Plotting\nplt.scatter(x, y, color='blue')\nplt.plot(x, y_pred, color='red')\nplt.title('Linear Regression Example')\nplt.xlabel('Independent variable (x)')\nplt.ylabel('Dependent variable (y)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=576 height=449}\n:::\n:::\n\n\nIn this code, we start by generating random data to simulate a real-world dataset. The **`LinearRegression`** class from Scikit-learn is used to fit the model, which involves finding the coefficients that minimize the difference between the predicted and actual values. Finally, the results are visualized in a plot, showing the data points and the fitted regression line, providing a clear illustration of the model's performance.\n\n## **Example 2: Multiple Linear Regression**\n\nOur second example extends to multiple linear regression, where we use more than one independent variable for prediction.\n\n### **Python Code for Multiple Linear Regression**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generating random data \nn = 100 \nnp.random.seed(0) \nx = np.random.rand(n,2) * 100  # Two independent variables \ny = 4 + 2 * x[:,0] + 3 * x[:,1] + np.random.rand(n) * 40  \n\n# Fitting the model \nmodel = LinearRegression() \nmodel.fit(x, y) \ny_pred = model.predict(x) \n# Plotting  \nplt.scatter(x[:,0], y, color='green', label='Independent Variable 1') \nplt.scatter(x[:,1], y, color='blue', label='Independent Variable 2') \nplt.title('Multiple Linear Regression Example') \nplt.xlabel('Independent Variables') \nplt.ylabel('Dependent Variable') \nplt.show()  \n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=593 height=449}\n:::\n:::\n\n\nIn this second example, we introduce an additional independent variable, making it a multiple linear regression model. This type of model can capture more complex relationships between variables. The plot here shows the relationship between each independent variable and the dependent variable. Notice the addition of a legend to differentiate between the two independent variables.\n\n## **Differences Between the Two Examples**\n\n1.  **Number of Independent Variables**: The first example uses a single independent variable, while the second uses two. This demonstrates the transition from simple to multiple linear regression.\n\n2.  **Complexity**: Multiple linear regression (Example 2) can model more complex relationships compared to simple linear regression (Example 1). It's more versatile in handling real-world scenarios where multiple factors influence the outcome.\n\n3.  **Visualization**: In the first example, we can visualize the relationship directly as a line. In the second, due to the addition of another dimension, the visualization becomes more complex, showing individual relationships with each independent variable.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}