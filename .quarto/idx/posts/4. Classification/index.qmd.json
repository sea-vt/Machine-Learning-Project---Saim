{"title":"4\\. Machine Learning in Classification","markdown":{"yaml":{"title":"4\\. Machine Learning in Classification","author":"Saim Ehtesham Ali","date":"2023-11-22","categories":["Classification","ML","ML Basics","ROC","PR","Confusion Matrix"],"output":"html_document","image":"lm 4.png","format":{"html":{"code-fold":true}},"code-fold":true,"keep-ipynb":true},"headingText":"**Machine Learning in Classification: ROC, PR, and Confusion Matrix**","containsRefs":false,"markdown":"\n\n\nClassification is a core task in machine learning where the objective is to predict discrete categories or class labels. To evaluate the performance of classification models, several tools are used, including ROC curves, PR curves, and confusion matrices. This blog post delves into these evaluation metrics and demonstrates their use with examples.\n\n## **ROC (Receiver Operating Characteristic)**\n\n### **Understanding ROC**\n\nThe ROC curve is a graphical representation of a classification model's performance. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. The Area Under the Curve (AUC) of the ROC curve is a single value summary of the model performance.\n\n### **Example: ROC Curve Using Iris Dataset**\n\n```{python}\n# Import necessary libraries\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binary classification problem: Classify Iris-setosa vs others\ny = (y == 0).astype(int) \n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nmodel = LogisticRegression(solver='lbfgs', max_iter=200)\nmodel.fit(X_train, y_train)\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Plotting ROC Curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n```\n\nIn this example, we use the Iris dataset to perform binary classification (Iris-setosa vs others). We train a logistic regression model and plot the ROC curve, along with calculating the AUC to evaluate the model's performance.\n\n## **PR (Precision-Recall)**\n\n### **Understanding PR**\n\nThe Precision-Recall (PR) curve is another tool for evaluating classifiers, especially useful in imbalanced datasets. It plots precision (positive predictive value) against recall (true positive rate).\n\n### **Example: PR Curve Using Breast Cancer Dataset**\n\n```{python}\n# Import libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Logistic Regression Model\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, y_train)\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Precision-Recall Curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Plotting\nplt.plot(recall, precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.show()\n\n```\n\nThis example uses the Breast Cancer dataset to demonstrate the PR curve. A logistic regression model is trained, and the PR curve is plotted to assess the trade-off between precision and recall for different threshold values.\n\n## **Confusion Matrix**\n\n### **Understanding Confusion Matrix**\n\nA confusion matrix is a table used to evaluate the performance of a classification model. It shows the actual versus predicted classifications and breaks down the results into true positives, true negatives, false positives, and false negatives.\n\n### **Example: Confusion Matrix Using Wine Dataset**\n\n```{python}\n# Import libraries\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load wine dataset\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a Random Forest Classifier\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Generating the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualization\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n```\n\nIn this example, we use the Wine dataset to demonstrate the confusion matrix. After training a Random Forest Classifier, the confusion matrix is plotted using Seaborn, providing a clear visualization of the model's performance in terms of correct and incorrect classifications.\n\n## **Concluding Thoughts**\n\nThe tools and examples discussed in this blog post provide a comprehensive overview of evaluating classification models in machine learning. The ROC and PR curves offer insights into the model's performance from different perspectives, particularly useful in varied dataset conditions. The confusion matrix, on the other hand, provides a straightforward, quantifiable measure of a model's predictive capabilities and errors.\n\nWhen applied correctly, these tools can greatly assist in understanding the strengths and weaknesses of classification models, guiding data scientists in model selection, tuning, and improvement. As machine learning continues to evolve, the proper evaluation of models remains a cornerstone of developing robust, reliable, and efficient predictive systems.\n\nRemember, the journey in machine learning is as much about understanding and interpreting the models as it is about building them.\n","srcMarkdownNoYaml":"\n\n# **Machine Learning in Classification: ROC, PR, and Confusion Matrix**\n\nClassification is a core task in machine learning where the objective is to predict discrete categories or class labels. To evaluate the performance of classification models, several tools are used, including ROC curves, PR curves, and confusion matrices. This blog post delves into these evaluation metrics and demonstrates their use with examples.\n\n## **ROC (Receiver Operating Characteristic)**\n\n### **Understanding ROC**\n\nThe ROC curve is a graphical representation of a classification model's performance. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. The Area Under the Curve (AUC) of the ROC curve is a single value summary of the model performance.\n\n### **Example: ROC Curve Using Iris Dataset**\n\n```{python}\n# Import necessary libraries\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binary classification problem: Classify Iris-setosa vs others\ny = (y == 0).astype(int) \n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nmodel = LogisticRegression(solver='lbfgs', max_iter=200)\nmodel.fit(X_train, y_train)\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Plotting ROC Curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n```\n\nIn this example, we use the Iris dataset to perform binary classification (Iris-setosa vs others). We train a logistic regression model and plot the ROC curve, along with calculating the AUC to evaluate the model's performance.\n\n## **PR (Precision-Recall)**\n\n### **Understanding PR**\n\nThe Precision-Recall (PR) curve is another tool for evaluating classifiers, especially useful in imbalanced datasets. It plots precision (positive predictive value) against recall (true positive rate).\n\n### **Example: PR Curve Using Breast Cancer Dataset**\n\n```{python}\n# Import libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Logistic Regression Model\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, y_train)\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Precision-Recall Curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Plotting\nplt.plot(recall, precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.show()\n\n```\n\nThis example uses the Breast Cancer dataset to demonstrate the PR curve. A logistic regression model is trained, and the PR curve is plotted to assess the trade-off between precision and recall for different threshold values.\n\n## **Confusion Matrix**\n\n### **Understanding Confusion Matrix**\n\nA confusion matrix is a table used to evaluate the performance of a classification model. It shows the actual versus predicted classifications and breaks down the results into true positives, true negatives, false positives, and false negatives.\n\n### **Example: Confusion Matrix Using Wine Dataset**\n\n```{python}\n# Import libraries\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load wine dataset\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a Random Forest Classifier\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Generating the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualization\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n```\n\nIn this example, we use the Wine dataset to demonstrate the confusion matrix. After training a Random Forest Classifier, the confusion matrix is plotted using Seaborn, providing a clear visualization of the model's performance in terms of correct and incorrect classifications.\n\n## **Concluding Thoughts**\n\nThe tools and examples discussed in this blog post provide a comprehensive overview of evaluating classification models in machine learning. The ROC and PR curves offer insights into the model's performance from different perspectives, particularly useful in varied dataset conditions. The confusion matrix, on the other hand, provides a straightforward, quantifiable measure of a model's predictive capabilities and errors.\n\nWhen applied correctly, these tools can greatly assist in understanding the strengths and weaknesses of classification models, guiding data scientists in model selection, tuning, and improvement. As machine learning continues to evolve, the proper evaluation of models remains a cornerstone of developing robust, reliable, and efficient predictive systems.\n\nRemember, the journey in machine learning is as much about understanding and interpreting the models as it is about building them.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":"html_document","warning":true,"include":true,"keep-md":false,"keep-ipynb":true,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"4\\. Machine Learning in Classification","author":"Saim Ehtesham Ali","date":"2023-11-22","categories":["Classification","ML","ML Basics","ROC","PR","Confusion Matrix"],"image":"lm 4.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}